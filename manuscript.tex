\documentclass[conference]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{xcolor}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{stfloats}
\usepackage[]{hyperref}
% Shortcuts
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table ~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}

\begin{document}
\title{Learning Effective Changes for  Software Projects\\[-0.8cm]}
\author{Rahul Krishna\\
	Comptuer Science, North Carolina State University, USA\\
	i.m.ralk@gmail.com}

% make the title area
\maketitle


\begin{abstract}
% Brief premise
% A common solution
% A challenge with this solution
% An alternative option
% The purpose of this thesis
% Validation plan

The primary motivation of much of software analytics is decision making. How do you make these decisions? Should one make decisions based on lessons that arise from within a particular project or should one generate these decisions from across multiple projects? If sufficient data is not available within a project, can practitioners learn lessons from other projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction algorithms. Indeed prediction is a useful task, but it is usually followed by ``planning'' about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics that offer clear guidance on \textit{what to do} within the context of a specific software project. Specifically, we propose the XTREE algorithm for generating a set of actionable plans. Each of these plans, if followed will improve the quality of the software project.

\end{abstract}

\begin{IEEEkeywords}
Data mining, actionable analytics, bellwethers, defect prediction.
\end{IEEEkeywords}


\section{Introduction}
\label{sect:intro}

% {\color{blue} 
% \be
% \item The goal of this research is to tame conclusion instability enough to be able to generate actionable analytics.
% \ee
% }
Over the past decade, advances in AI have enabled a widespread use of data analytics in software engineering. For example, we can now estimate	how long it would take to integrate the new code~\cite{czer11}, where bugs are most likely to occur~\cite{ostrand04,Menzies2007a}, or how much effort it will take to develop a software package~\cite{turhan11,koc11b}, etc. Despite these successes, there are two primary operational shortcomings with many software analytic tools: (a) Conclusion instability as a result of constant influx of new data; and (b) Lack of insightful analytics.

In several applications, local data is scarce. Researchers of transfer learning report that the use of data from other projects can yield comparable predictors to just using local data~\cite{peters15}. However, new projects are constantly being created and as Rahman et al.~\cite{rahman12} warn, if quality predictors are always being updated based on the specifics of new data, then those new predictors may suffer from over-fitting. Such over-fitted models are ``brittle'' in the sense that they can undergo constant changes whenever new data arrives and leads to unstable conclusions. Conclusion instability is unsettling for software project managers struggling to find general policies.
We seek methods to support those managers, who seek stability
in their conclusions, while also allowing new projects
to take full benefit from data arriving
from all the other projects. Our research has offered strong evidence that organizations can declare some prior project as  the {\em ``bellwether''}  \footnote{According to the Oxford English Dictionary, the ``bellwether'' is the leading sheep of a flock, with a bell on its neck.} that can then offer predictions that generalize across $N$ projects.


Business users lament that most software analytics tools, ``Tell us what \textit{is}. But they don't tell us \textit{what to do}''. A similar concern was raised by several researchers at a recent workshop on ``Actionable Analytics'' at 2015 IEEE conference on Automated Software Engineering~\cite{hihn15}. 

For example, most software analytics tools in the area of detecting software defects are mostly \textit{prediction} algorithms such as support vector machines~\cite{cortes95}, naive bayes classifiers~\cite{lessmann08}, logistic regression~\cite{lessmann08}, decision trees~\cite{dtrees}, etc. These prediction algorithms report what combinations of software project features predict for say the number of defects. But this is different task to \textit{planning}, which answers a more pressing question: what to {\em change} in order to {\em improve} quality. Accordingly, in this research, we seek tools that offer clear guidance on what to do in a specific project.

The tool assessed in this paper is the XTREE \textit{planning} tool~\cite{krishna17a}. XTREE employs a $cluster+contrast$ approach to planning where it (a) \textit{Clusters} different parts of the software project based on a quality measure (e.g. the number of defects); (b) Reports the \textit{contrast sets} between neighboring clusters. Each of these contrast sets represent the difference between these clusters and they can  be interpreted as plans, i.e., 
\begin{itemize}
	    \item If your current project falls into cluster $C_1$,
	    \item Some neighboring cluster $C_2$ has better quality.
	    \item Then the difference {\em $\Delta=$ $C_2$ - $C_1$} is a {\em plan} for changing a  project such that it \textit{might} have   higher quality.
\end{itemize}

\section{Contributions of this work}
\label{sect:contributions}

The previous section introduced two current problems in software engineering. To address these, this thesis will make the following contributions:

\textit{1. New kinds of software analytics techniques:} This research introduces the notion of planning in software engineering. In addition to showing that planning in effective in a within-project setting~\cite{krishna17a}, we also show that with bellwethers~\cite{krishna16}, these plans can be translated to cross-project problems with encouraging results. 
This is a unique approach that combines our efforts to address the problems highlighted in \tion{intro}. Our previous work on bellwethers~\cite{krishna16} explored the possibility of taming the conclusion instability in defect prediction. Concurrently, the our work on planning~\cite{krishna17a} explored the generation of actionable plans for within-project problems. 

\textit{2. Compelling results of planning:} Our results have established that planning is quite successful in producing actions that can reduce the number of defects. In~\fig{result1}, we have shown that planning can reduce defects by more than 50\% in 6 out of the 8 datasets studied there ($>$90\% in the certain cases).

\textit{3. Evidence of generality of bellwethers:}
The more the bellwether effect is explored, the more we learn about its broad applicability. Originally, we explored this just in the context of prediction~\cite{krishna16}, it has been shown to work for defect prediction, effort estimation, predicting when issues will close, and detecting code smells~\cite{krishna17b}. Our preliminary results reported in this work show that bellwethers can also be used from cross-project quality planning. This is an important result of much significance since, where bellwethers occur, reasoning about multiple software projects becomes a simple matter of adding a few for-loops around existing within-project methods.

\textit{4. Replication Package:} For readers this work who wish to replicate our findings, we have made available a replication package at \url{https://git.io/v7c9k}. 


\section{Research Questions}

In this work we investigate the following research questions:

\noindent\textit{RQ1. How prevalent are bellwether datasets?}
It is important to establish the prevalence of bellwethers first as this determines if it is possible to learn plans from the bellwether data. If bellwethers occur infrequently, we cannot rely on them for planning. To answer this question, in our previous work~\cite{krishna17b}, we explored four sub-domains within software engineering namely, defect prediction, effort estimation, issue lifetime estimation, and detection of code smells. In a result consistent with bellwethers being \textit{very} prevalent, we found that all these domains have a bellwether dataset.


\noindent\textit{RQ2. Does within-project planning with XTREE offer significant improvements in reducing defects?}
This research question seeks to establish if our preferred planning tool (XTREE) is effective in generating actionable plans. Our initial findings showed that XTREE was indeed an effective planner that can generate plans that are also succinct and stable. Further, these plans are not subject to conjunctive fallacy~\cite{krishna17a}.

\noindent\textit{RQ3. Does  cross-project  planning  with  BELLTREE offer significant improvements in reducing defects?}
Having established the prevalence of bellwether datasets and the efficacy of planning with XTREE, here we ask if it is possible for us to transfer plans across projects using the bellwether data and XTREE (referred to as BELLTREE). Our preliminary results are very encouraging. We show that the combining bellwethers and XTREE can lead to a very effective cross-project planner.


\noindent\textit{RQ4. Are cross-project  plans  any  better  than within project plans?}
This research question assesses the quality of plans obtained using XTREE and BELLTREE. This is important because within-project data is not always available (especially if a project is in it's early stage of development) and it may be useful to look to other similar projects for planning. Our preliminary results have suggested that the effectiveness of plans generated from within project data and XTREE is statistically comparable to plans derived cross-project data and BELLTREE. Thus, when project specific data is not available, one may use cross-project data to derive actionable plans.

\section{Related Work}

Planning  has been a subject of much research in classical artificial intelligence. Here, planning usually refers to generating a sequence of actions that enables an \textit{agent} to achieve a specific \textit{goal}~\cite{norvig}. This can be achieved by classical search-based problem solving  approaches or logical planning agents. Such planning tasks now play a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge~\cite{ghallab04}. Some of the most common planning paradigms include: (a) classical planning~\cite{wooldridge95}; (b) probabilistic planning~\cite{ghallab04, Bel, altman99, guo2009, kaelbling98}; and (c) preference-based planning~\cite{son06, baier09}. 

Existence of a model precludes the use of each of these planning approaches. This is a limitation of all these planning approaches since not every domain has a reliable model. In software engineering, the planning problem translates to proposing changes to software artifacts. These are usually a hybrid task combining elements of probabilistic planning and preference based plans. Solving this has been undertaken via the use of some search-based software engineering techniques~\cite{Harman2009, Harman2011}. Examples of algorithms include GALE, NSGA-II, NSGA-III, SPEA2, IBEA, MOEA/D, etc.~\cite{krall2015gale,deb00a,zit02,zit04, deb14,Cui2005a,zhang07:TEC}.

These search-based software engineering techniques all require access to some trustworthy models that can be used to explore some highly novel examples. In some software engineering domains there is ready access to such models which can offer assessment of newly generated plans. Examples of such domains within software engineering include automated program repair~\cite{Weimer2009, LeGoues2015}, software product line management~\cite{sayyad13, metzger14, henard15}, automated test generation~\cite{me09m,andrews10}, etc.  

However, not all domains come with ready-to-use models. For example, consider software defect prediction and all the intricate issues that may lead to defects in a product. A model that includes {\em all} those potential issues would be very large and complex. Further, the empirical data required to validate any/all parts of that model can be hard to find. Also, even when there is an existing model, they can require constant  maintenance lest they become out-dated. These problems are they key motivations for us to look for alternate methods that can be automatically updated with new data.

In summary, for domains with readily accessible models, we recommend
the tools widely used in the search-based
software engineering community such as GALE, NSGA-II, NSGA-III, SPEA2, IBEA, particle swarm optimization, MOEA/D, etc. In cases where this is not an option, we propose the use of data mining approaches to create a quasi-model of the domain 
and make of use observable states from this data to generate an estimation of the model. Our preferred tools in this paper XTREE and BELLTREE take this approach and as presented elsewhere in this paper, these methodologies have very encouraging results.

\section{Planning in Software Engineering}

We distinguish planning from prediction for software quality as follows: 
Quality prediction points to the likelihood of defects. Predictors take the form:

\begin{equation*}
    out = f(in)    
\end{equation*}

where $in$ contains many independent features and out contains some measure of
how many defects are present. For software analytics, the function $f$ is learned via data mining (for static code attributes for instance).

On the other hand, quality planning generates a concrete set of actions that can be taken (as precautionary measures) to significantly reduce the likelihood of defects occurring in the future.

For a formal definition of plans, consider a test example $Z$, planners
proposes a plan $D$ to adjust attribute $Z_j$ as follows:

{\small\[
\forall \delta_j \in \Delta :  Z_j =  
\begin{cases}
     Z_j + \delta_j& \text{if $Z_j$ is numeric}\\
    \delta_j              & \text{otherwise}
\end{cases}
\]}

With this planner, to (say) simplify a large bug-prone method, our planners
might suggest to a developer to reduce its size (i.e. refactor that
code by, say, splitting it across two simpler functions).

The rest of this section details how we may acheive this. Specifically, we recommend the use of the XTREE planner~\cite{krishna17a}.
\subsection{XTREE}
XTREE builds a decision tree,  then generates
plans by contrasting the differences between two branches:
(1)~the branch where you are; (2)~the branch to where you want to be.

One potential problem with the other three methods is the {\em unsupervised} nature of
their plans. These  
execute without knowledge of the target class (defects).  {\em Supervised} methods assume that it is useful to also reflect on the target class.

XTREE uses a supervised decision tree algorithm of \fig{xtree}.A to divide the data.
Next, XTREE builds plans from the branches of the decision trees using the description of \fig{xtree}.C.
In doing so, we ask three questions, the last of which returns the plan:
\be
\item
Which {\em current} branch does a test case fall in?
\item Which {\em desired} branch would the test case want to move to?
\item What are the {\em deltas} between current and desired? 
\ee
These \textit{deltas} represent the threshold ranges\footnote{Thresholds are denoted by $[low,high)$ ranges for each metric} that represent the plans to reduce the defects. 


\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{manuscript}

\end{document}