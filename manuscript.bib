@INPROCEEDINGS{czer11,
	author =	 {Czerwonka, J. and Das, R. and Nagappan, N. and
	Tarvo, A. and Teterev, A.},
	booktitle =	 {Software Testing, Verification and Validation
	(ICST), 2011 IEEE Fourth International Conference
	on},
	title =	 {CRANE: Failure Prediction, Change Analysis and Test
	Prioritization in Practice -- Experiences from
	Windows},
	year =	 2011,
	month =	 {march},
	pages =	 {357 -366},
}

@book{norvig,
  title={Artificial Intelligence: A Modern Approach},
  author={Russell, Stuart and Norvig, Peter},
  publisher={Prentice-Hall, Egnlewood Cliffs},
  isbn={0-13-604259-7},
  year={1995}
}

@book{ghallab04,
  title={Automated Planning: theory and practice},
  author={Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  year={2004},
  publisher={Elsevier}
}

@ARTICLE{Bel,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
     coden = "IUMJAB",
   mrclass = "",
}

@article{kaelbling98,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@article{guo2009,
  title={Continuous-time Markov decision processes},
  author={Guo, Xianping and Hern{\'a}ndez-Lerma, On{\'e}simo},
  journal={Continuous-Time Markov Decision Processes},
  pages={9--18},
  year={2009},
  publisher={Springer}
}

@book{altman99,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@article{wooldridge95,
  title={Intelligent agents: Theory and practice},
  author={Wooldridge, Michael and Jennings, Nicholas R},
  journal={The knowledge engineering review},
  volume={10},
  number={2},
  pages={115--152},
  year={1995},
  publisher={Cambridge University Press}
}
@inproceedings{baier09,
  title={HTN planning with preferences},
  author={Baier, Shirin Sohrabi Jorge A and McIlraith, Sheila A},
  booktitle={21st Int. Joint Conf. on Artificial Intelligence},
  pages={1790--1797},
  year={2009}
}

@article{son06,
  title={Planning with preferences using logic programming},
  author={Son, Tran Cao and Pontelli, Enrico},
  journal={Theory and Practice of Logic Programming},
  volume={6},
  number={5},
  pages={559--607},
  year={2006},
  publisher={Cambridge University Press}
}

@inproceedings{ostrand04,
	author =	 {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell,
	Robert M.},
	title =	 {Where the bugs are},
	booktitle =	 {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT
	international symposium on Software testing and
	analysis},
	year =	 2004,
	pages =	 {86--96},
	publisher =	 {ACM},
	address =	 {New York, NY, USA},
}

@article{Menzies2007a,
	abstract = {Zhang and Zhang argue that predictors are useless unless they have high precison{\&}amp;recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.},
	author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
	doi = {10.1109/TSE.2007.70721},
	file = {:C$\backslash$:/Users/Rahul Krishna/Documents/Mendeley Desktop/Menzies et al. - 2007 - Problems with precision A response to Comments on 'data mining static code attributes to learn defect predictors.pdf:pdf},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Accuracy measures,Defect prediction,Empirical,Static code attributes},
	number = {9},
	title = {{Problems with Precision: A Response to "Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'"}},
	volume = {33},
	year = {2007}
}
@inproceedings{turhan11,
	title={Empirical evaluation of mixed-project defect prediction models},
	author={Turhan, Burak and Tosun, Ay{\c{s}}e and Bener, Ay{\c{s}}e},
	booktitle={Software Engineering and Advanced Applications (SEAA), 2011 37th EUROMICRO Conference on},
	pages={396--403},
	year={2011},
	organization={IEEE}
}

@ARTICLE{koc11b,
	author =	 "E. Kocaguneli and T. Menzies and A. Bener and
	J. Keung",
	journal =	 "IEEE Transactions on Software Engineering",
	title =	 "Exploiting the Essential Assumptions of
	Analogy-Based Effort Estimation",
	year =	 2012,
	volume =	 28,
	issue =	 2,
	pages =	 "425-438",
	class =	 "hJ"
}

@inproceedings{hihn15, 
author={J. Hihn and T. Menzies}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)}, 
title={Data Mining Methods and Cost Estimation Models: Why is it So Hard to Infuse New Ideas?}, 
year={2015}, 
pages={5-9}, 
keywords={data mining;software cost estimation;NASA;cost estimation models;data mining methods;effort estimation;software costing models;technology infusion;Cognitive science;Data mining;Estimation;NASA;Software;Solid modeling;Stakeholders;cost estimation;data mining;effort estimation;software;technology infusion}, 
doi={10.1109/ASEW.2015.27}, 
month={Nov},}

@article{cortes95,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer Netherlands}
}

@article{ck,
  title={A metrics suite for object oriented design},
  author={Chidamber, Shyam R and Kemerer, Chris F},
  journal={IEEE Transactions on software engineering},
  volume={20},
  number={6},
  pages={476--493},
  year={1994},
  publisher={IEEE}
}

@article{lessmann08,
	abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
	author = {Lessmann, Stefan and Baesens, Bart and Mues, C. and Pietsch, S.},
	doi = {10.1109/TSE.2008.35},
	isbn = {00985589 (ISSN)},
	issn = {0098-5589},
	journal = {IEEE Trans. Softw. Eng.},
	keywords = {Complexity measures,Data mining,Formal methods,Statistical methods,benchmark testing,benchmarking classification models,code attributes,fault-prone modules,metric-based classification,predictive classification models,proprietary data sets,software defect prediction,software quality,statistical testing,statistical testing procedures,testing efficiency},
	month = {jul},
	number = {4},
	pages = {485--496},
	title = {{Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings}},
	volume = {34},
	year = {2008}
}

@article{dtrees,
  title={Induction of decision trees},
  author={Quinlan, J Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@article{krishna17a,
	abstract = {Context: Developers use bad code smells to guide code reorganization. Yet developers, text books, tools, and researchers disagree on which bad smells are important. Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a tool that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, and the bad smell recommendations (learned from previous studies) are not universal to all software projects. Conclusion: Before undertaking a code reorganization based on a bad smell report, use a tool like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods. This recommendation assumes that there is an historical record. If none exists, then the results of this paper could be used as a guide.},
	archivePrefix = {arXiv},
	arxivId = {1609.03614},
	author = {Krishna, Rahul and Menzies, Tim and Layman, Lucas},
	doi = {10.1016/j.infsof.2017.03.012},
	eprint = {1609.03614},
	issn = {09505849},
	journal = {Information and Software Technology},
	keywords = {bad smells,decision trees,performance prediction},
	month = {mar},
	title = {{Less is more: Minimizing code reorganization using XTREE}},
	year = {2017}
}

@inproceedings{peters15,
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.92},
isbn = {9781479919345},
issn = {02705257},
pages = {801--811},
title = {{LACE2: Better privacy-preserving data sharing for cross project defect prediction}},
volume = {1},
year = {2015}
}

@inproceedings{rahman12,
 author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
 title = {Recalling the "Imprecision" of Cross-project Defect Prediction},
 booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
 series = {FSE '12},
 year = {2012},
 isbn = {978-1-4503-1614-9},
 location = {Cary, North Carolina},
 pages = {61:1--61:11},
 articleno = {61},
 numpages = {11},
 doi = {10.1145/2393596.2393669},
 acmid = {2393669},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical software engineering, fault prediction, inspection},
}

@inproceedings{krishna16,
	abstract = {Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.},
	address = {New York, New York, USA},
	author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
	booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering - ASE 2016},
	doi = {10.1145/2970276.2970339},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Krishna, Menzies, Fu - 2016 - Too much automation the bellwether effect and its implications for transfer learning.pdf:pdf},
	isbn = {9781450338455},
	keywords = {data mining,defect prediction,transfer learning},
	mendeley-groups = {Bellwether Journal},
	pages = {122--131},
	publisher = {ACM Press},
	title = {{Too much automation? the bellwether effect and its implications for transfer learning}},
	year = {2016}
}
@article{krishna17b,
	abstract = {Transfer learning has been the subject of much recent research. In practice, that research means that the models are unstable since they are continually revised whenever new data arrives. This paper offers a very simple "bellwether" transfer learner. Given N datasets, we find which one produces the best predictions on all the others. This bellwether dataset is then used for all subsequent predictions (when its predictions start failing, one may seek another bellwether). Bellwethers are interesting since they are very simple to find (wrap a for-loop around standard data miners). They simplify the task of making general policies in software engineering since as long as one bellwether remains useful, stable conclusions for {\$}N{\$} datasets can be achieved by reasoning over that bellwether. This paper shows that this bellwether approach works for multiple datasets from various domains in SE. From this, we conclude that (1) bellwether method is a useful (and simple) transfer learner; (2) Unlike bellwethers, other complex transfer learners do not generalized to all domains in SE; (3) "bellwethers" are a baseline method against which future transfer learners should be compared; (4) When building increasingly complex automatic methods, researchers should pause and compare more sophisticated method against simpler alternatives.},
	archivePrefix = {arXiv},
	arxivId = {1703.06218},
	author = {Krishna, Rahul and Menzies, Tim},
	journal ={TSE (under review)},
	eprint = {1703.06218},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Krishna, Menzies - 2017 - Simpler Transfer Learning (Using Bellwethers).pdf:pdf},
	month = {mar},
	pages = {1--18},
	title = {{Simpler Transfer Learning (Using "Bellwethers")}},
	url = {http://arxiv.org/abs/1703.06218},
	year = {2017}
}


@article{deb14,
author = {Deb, K and Jain, H},
doi = {10.1109/TEVC.2013.2281535},
file = {:Users/timm/svns/doc/13nsga-III.pdf:pdf},
issn = {1089-778X},
journal = {Evolutionary Computation, IEEE Transactions on},
keywords = {genetic algorithms;sorting;EMO algorithms;MOEA/D m},
month = aug,
number = {4},
pages = {577--601},
title = {{An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints}},
volume = {18},
year = {2014}
}

@article{zhang07:TEC,
author = {Zhang, Qingfu and Li, Hui},
doi = {10.1109/TEVC.2007.892759},
issn = {1089-778X},
journal = {Evolutionary Computation, IEEE Transactions on},
keywords = {computational complexity;genetic algorithms;comput},
month = dec,
number = {6},
pages = {712--731},
title = {{MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition}},
volume = {11},
year = {2007}
}

@article{krall2015gale,
  title={Gale: Geometric active learning for search-based software engineering},
  author={Krall, Joseph and Menzies, Tim and Davies, Misty},
  journal={IEEE Transactions on Software Engineering},
  volume={41},
  number={10},
  pages={1001--1018},
  year={2015},
  publisher={IEEE}
}

@incollection{zit04,
author = {Zitzler, Eckart and K\"{u}nzli, Simon},
booktitle = {Parallel Problem Solving from Nature - PPSN VIII},
doi = {10.1007/978-3-540-30217-9\_84},
editor = {Yao, Xin and Burke, EdmundK. and Lozano, Jos\'{e}A. and Smith, Jim and Merelo-Guerv\'{o}s, JuanJuli\'{a}n and Bullinaria, JohnA. and Rowe, JonathanE. and Tiňo, Peter and Kab\'{a}n, Ata and Schwefel, Hans-Paul},
file = {:Users/timm/svns/doc/04zitzlerIBEA.pdf:pdf},
isbn = {978-3-540-23092-2},
pages = {832--842},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Indicator-Based Selection in Multiobjective Search}},
volume = {3242},
year = {2004}
}

@inproceedings{zit02,
author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
booktitle = {Evolutionary Methods for Design, Optimisation, and Control},
publisher = {CIMNE, Barcelona, Spain},
title = {{SPEA2: Improving the Strength Pareto Evolutionary Algorithm for Multiobjective Optimization}},
year = {2002}
}

@article{deb00a,
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
journal = {IEEE Transactions on Evolutionary Computation},
pages = {182--197},
title = {{A Fast Elitist Multi-Objective Genetic Algorithm: NSGA-II}},
volume = {6},
year = {2002}
}

@article{Cui2005a,
author = {Cui, X and Potok, Te and Palathingal, P},
file = {:Users/timm/svns/doc/pso/05clusterPSO.pdf:pdf},
journal = {\ldots  Intelligence Symposium, 2005. \ldots},
title = {{Document clustering using particle swarm optimization}},
year = {2005}
}

@article{Harman2009,
  title={Search based software engineering: A comprehensive analysis and review of trends techniques and applications},
  author={Harman, Mark and Mansouri, S Afshin and Zhang, Yuanyuan},
  journal={Dept. Comp. Sci, King’s College London, Tech. Rep. TR-09-03},
  year={2009}
}

@article{Harman2011,
abstract = {The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. The idea is to exploit humans creativity and machines tenacity and reliability, rather than requiring humans to perform the more tedious, error prone and thereby costly aspects of the engineering process. SBSE can also provide insights and decision support. This tutorial will present the reader with a step-by-step guide to the application of SBSE techniques to Software Engineering. It assumes neither previous knowledge nor experience with Search Based Optimisation. The intention is that the tutorial will cover sufficient material to allow the reader to become productive in successfully applying search based optimisation to a chosen Software Engineering problem of interest.},
author = {Harman, M and McMinn, P and {De Souza}, JT and Yoo, S},
doi = {10.1007/978-3-642-25231-0\_1},
journal = {Search},
pages = {1--59},
title = {{Search based software engineering: Techniques, taxonomy, tutorial}},
volume = {2012},
year = {2011}
}

@article{LeGoues2015,
author = {{Le Goues}, Claire and Holtschulte, Neal and Smith, Edward K and Brun, Yuriy and Devanbu, Premkumar and Forrest, Stephanie and Weimer, Westley},
doi = {10.1109/TSE.2015.2454513},
file = {:home/rkrsn/Documents/Mendeley Desktop/Le Goues et al. - 2015 - The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Automated program repair,INTROCLASS,MANYBUGS,benchmark,reproducibility,subject defect},
mendeley-groups = {Automated Program Repair},
month = {dec},
number = {12},
pages = {1236--1256},
title = {{The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs}},
volume = {41},
year = {2015}
}

@inproceedings{Weimer2009,
abstract = {Automatic program repair has been a longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a program fault is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs totaling 63,000 lines in under 200 seconds, on average.},
author = {Weimer, Westley and Nguyen, ThanhVu and {Le Goues}, Claire and Forrest, Stephanie},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2009.5070536},
file = {:home/rkrsn/Documents/Mendeley Desktop/Weimer, Goues, Forrest - 2009 - Automatically Finding Patches Using Genetic Programming ∗.pdf:pdf},
isbn = {9781424434527},
issn = {02705257},
keywords = {[Electronic Manuscript]},
mendeley-groups = {Automated Program Repair},
pages = {364--374},
publisher = {IEEE},
title = {{Automatically finding patches using genetic programming}},
year = {2009}
}


@inproceedings{sayyad13,
  title={Scalable product line configuration: A straw to break the camel's back},
  author={Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
  booktitle={Automated Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on},
  year={2013},
  organization={IEEE}
}

@INPROCEEDINGS{henard15, 
author={C. Henard and M. Papadakis and M. Harman and Y. Le Traon}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={Combining Multi-Objective Search and Constraint Solving for Configuring Large Software Product Lines}, 
year={2015}, 
volume={1}, 
pages={517-528}, 
keywords={configuration management;optimisation;search problems;software metrics;software product lines;SATIBEA framework;SPL feature selection;constraint solving;diversity metrics;multiobjective search-based optimization;software product lines configuration;Filtering algorithms;Frequency modulation;Measurement;Optimization;Search problems;Software;Software product lines}, 
doi={10.1109/ICSE.2015.69}, 
ISSN={0270-5257}, 
month={May},}

@inproceedings{metzger14,
  title={Software product line engineering and variability management: achievements and challenges},
  author={Metzger, Andreas and Pohl, Klaus},
  booktitle={Proceedings of the on Future of Software Engineering},
  pages={70--84},
  year={2014},
  organization={ACM}
}

@inproceedings{me09m,
annote = {Available from  http://menzies.us/pdf/09fssga.pdf},
author = {Andrews, Jamie and Menzies, Tim},
booktitle = {PROMISE'09},
title = {{On the Value of Combining Feature Subset Selection with Genetic Algorithms: Faster Learning of Coverage Models}},
year = {2009}
}


@article{andrews10,
author = {Andrews, James H and Menzies, Tim and Li, Felix C H},
journal = {IEEE Transactions on Software Engineering},
month = mar,
title = {{Genetic Algorithms for Randomized Unit Testing}},
year = {2010}
}


@inproceedings{Cheng10,
 author = {Cheng, Betty and Jensen, Adam   },
 title = {On the Use of Genetic Programming for Automated Refactoring and the Introduction of Design Patterns},
 booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '10},
 year = {2010},
 isbn = {978-1-4503-0072-8},
 location = {Portland, Oregon, USA},
 pages = {1341--1348},
 numpages = {8},
 doi = {10.1145/1830483.1830731},
 acmid = {1830731},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design patterns, evolutionary computation, intelligent search, object-oriented design, refactoring, search-based software engineering, software metrics},
}

@article{OKeeffe08,
 author = {O'Keeffe, Mark and Cinn{\'e}ide, Mel \'{O}},
 title = {Search-based Refactoring: An Empirical Study},
 journal = {J. Softw. Maint. Evol.},
 issue_date = {September 2008},
 volume = {20},
 number = {5},
 month = sep,
 year = {2008},
 issn = {1532-060X},
 pages = {345--364},
 numpages = {20},
 url = {http://dx.doi.org/10.1002/smr.v20:5},
 doi = {10.1002/smr.v20:5},
 acmid = {1416585},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
 keywords = {automated design improvement, refactoring tools, search-based software engineering},
} 



@inproceedings{OKeeffe07,
 author = {O'Keeffe, Mark Kent and Cinneide, Mel O.},
 title = {Getting the Most from Search-based Refactoring},
 booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '07},
 year = {2007},
 isbn = {978-1-59593-697-4},
 location = {London, England},
 pages = {1114--1120},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1276958.1277177},
 doi = {10.1145/1276958.1277177},
 acmid = {1277177},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated design improvement, object-oriented product metrics, refactoring, search-based software engineering},
} 

@Inbook{Moghadam2011,
author="Moghadam, Iman Hemati",
chapter="Multi-level Automated Refactoring Using Design Exploration",
title="Search Based Software Engineering: Third International Symposium, SSBSE 2011, Szeged, Hungary, September 10-12, 2011. Proceedings",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="70--75",
isbn="978-3-642-23716-4",
doi="10.1007/978-3-642-23716-4_9",
url="http://dx.doi.org/10.1007/978-3-642-23716-4_9"
}

@inproceedings{Mkaouer14,
 author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy and \'{O} Cinn{\'e}ide, Mel},
 title = {Recommendation System for Software Refactoring Using Innovization and Interactive Dynamic Optimization},
 booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
 series = {ASE '14},
 year = {2014},
 isbn = {978-1-4503-3013-8},
 location = {Vasteras, Sweden},
 pages = {331--336},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2642937.2642965},
 doi = {10.1145/2642937.2642965},
 acmid = {2642965},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {refactoring, search based software engineering, software quality},
} 

@article{Bansiya02,
 author = {Bansiya, Jagdish and Davis, Carl G.},
 title = {A Hierarchical Model for Object-Oriented Design Quality Assessment},
 journal = {IEEE Trans. Softw. Eng.},
 issue_date = {January 2002},
 volume = {28},
 number = {1},
 month = jan,
 year = {2002},
 issn = {0098-5589},
 pages = {4--17},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/32.979986},
 doi = {10.1109/32.979986},
 acmid = {513066},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {quality model, quality attributes, design metrics, product metrics, object-oriented metrics},
} 

@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. Keywords:},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023\%2FA\%3A1010933404324},
author = {Breiman, L},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023\%2FA\%3A1010933404324},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
year = {2001}
}

@inproceedings{Nam13,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. © 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2013.6606584},
isbn = {9781467330763},
issn = {02705257},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}

@inproceedings{Nam15,
address = {New York, New York, USA},
author = {Nam, Jaechang and Kim, Sunghun},
booktitle = {Proc. 2015 10th Jt. Meet. Found. Softw. Eng. - ESEC/FSE 2015},
doi = {10.1145/2786805.2786814},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Nam, Kim - Heterogeneous defect prediction - 2015.pdf:pdf},
isbn = {9781450336758},
keywords = {defect prediction,heterogeneous metrics,quality assurance},
mendeley-groups = {ASE 2016},
pages = {508--519},
publisher = {ACM Press},
title = {{Heterogeneous defect prediction}},
year = {2015}
}

@article{fu,
 title="Tuning for Software Analytics: is it Really Necessary?",
author="W. Fu and T. Menzies and X. Shen",
journal="Information and Software Technology (submitted)",
year=2016
}

@inproceedings{nair2016accidental,
 title={An (accidental) exploration of alternatives to evolutionary algorithms for sbse},
 author={Nair, Vivek and Menzies, Tim and Chen, Jianfeng},
 booktitle={International Symposium on Search Based Software Engineering},
 pages={96--111},
 year={2016},
 organization={Springer}
}
@article{localvsglobal,
	abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data. [ABSTRACT FROM PUBLISHER]},
	author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
	doi = {10.1109/TSE.2012.83},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Menzies et al. - 2013 - Local versus Global Lessons for Defect Prediction and Effort Estimation.pdf:pdf},
	isbn = {9781605580791},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Data mining,clustering,defect prediction,effort estimation},
	month = {jun},
	number = {6},
	pages = {822--834},
	title = {{Local versus Global Lessons for Defect Prediction and Effort Estimation}},
	url = {http://ieeexplore.ieee.org/document/6363444/},
	volume = {39},
	year = {2013}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}